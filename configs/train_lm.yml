# ====================
# Run Configuration
# ====================
run: 
  name: TransformerLM_valid
  runs_dir: ./runs/lm

# ====================
# Training Parameters
# ====================
train:
  steps: 20000
  lr: 0.0003

# ====================
# Model Configuration
# ====================
model:
  d_model: 512
  num_heads: 4
  num_decoder_layers: 4
  dropout: 0.2

# ====================
# Dataset Configuration
# ====================
data:
  dataset: tiny_shakespeare
  batch_size: 32
  context_length: 256

# ====================
# Tokenization Configuration
# ====================
tokenizer:
  vocab_size: 4096
  vocab_path: null

# ====================
# Sampling Configuration
# ====================
sampling:
  prompts: [
    "ROMEO:\n",
    "JULET:\n",
    "CAPULET:\nWhat say you now?"
  ]
  max_tokens: 256
  multinomial: true
  temperature: 0.8

# ====================
# Logging Configuration
# ====================
logging:

  wandb:
    enable: true
    save_ckpt: 0

  loss_steps: 10
  valid_steps: 500
  ckpt_steps: 5000
  sample_steps: 500